# Story 3.1: Model Listing Endpoint Implementation

## Status
Approved

## Story
**As a** developer using Ollama SDK,
**I want** to retrieve available models via the /api/tags endpoint,
**so that** I can see which models are available for use without changing my code

## Acceptance Criteria

1. GET /api/tags endpoint is implemented and returns 200 status
2. Endpoint fetches model list from OpenAI using the OpenAI SDK
3. OpenAI model response is translated to Ollama tags format
4. All required Ollama fields are populated (with dummy data where necessary)
5. Endpoint handles OpenAI API errors gracefully with proper error translation
6. Response format matches what Ollama SDK expects exactly
7. Structured logging is used for all operations
8. Integration tests pass using actual Ollama SDK

## Tasks / Subtasks

- [ ] Create tags endpoint handler (AC: 1)
  - [ ] Create `app/handlers/tags.py` file
  - [ ] Define async function `get_tags()` 
  - [ ] Add route decorator `@router.get("/api/tags")`
  - [ ] Add to main router in `app/main.py`

- [ ] Implement OpenAI model fetching (AC: 2)
  - [ ] Import OpenAI client wrapper from `app/clients/openai_client.py`
  - [ ] Call `list_models()` method to get OpenAI models
  - [ ] Add proper async/await handling
  - [ ] Include structured logging for the API call

- [ ] Create response translation logic (AC: 3, 4, 6)
  - [ ] Create translation function in `app/translators/response.py`
  - [ ] Implement `translate_models_list(openai_models) -> OllamaTagsResponse`
  - [ ] Map OpenAI model fields to Ollama format
  - [ ] Add dummy metadata for missing Ollama fields (size, quantization, etc.)
  - [ ] Ensure response structure matches Ollama SDK expectations

- [ ] Implement error handling (AC: 5)
  - [ ] Catch OpenAI SDK exceptions in handler
  - [ ] Use error translation from `app/utils/errors.py`
  - [ ] Return proper Ollama-formatted error responses
  - [ ] Log errors with full context using structlog

- [ ] Add request/response logging (AC: 7)
  - [ ] Log incoming request with correlation ID
  - [ ] Log OpenAI API call details
  - [ ] Log translation process
  - [ ] Log final response (without sensitive data)

- [ ] Write unit tests (AC: 8)
  - [ ] Create `tests/unit/test_tags_handler.py`
  - [ ] Test successful model listing
  - [ ] Test error scenarios (API down, invalid response, etc.)
  - [ ] Test translation logic with various model types
  - [ ] Mock OpenAI client for unit tests

- [ ] Write integration tests (AC: 8)
  - [ ] Create `tests/integration/test_tags_integration.py`
  - [ ] Use actual Ollama SDK to call endpoint
  - [ ] Verify response can be parsed by Ollama SDK
  - [ ] Test with mocked OpenAI responses from `references/openai-examples/`
  - [ ] Ensure 80% code coverage minimum

## Dev Notes

### Architecture Context
[Source: architecture/components.md#FastAPI-Server]
- The FastAPI server handles `/api/tags` endpoint routing
- Dependencies: Request Validator, Request/Response Translators, OpenAI Client Wrapper

[Source: architecture/components.md#Translation-Layer]
- Key interface: `translate_models_list(openai_models) -> OllamaTagsResponse`
- Uses Model Mapping configuration and structlog for warnings

[Source: architecture/components.md#OpenAI-Client-Wrapper]
- Key interface: `list_models() -> List[Model]`
- Uses OpenAI SDK 1.12.0 with HTTPX 0.26.0

[Source: architecture/core-workflows.md#List-Models-Workflow]
- Workflow: SDK → FastAPI → OpenAI Client → OpenAI API → Response Translator → SDK
- Translation includes adding required metadata

[Source: architecture/external-apis.md#OpenAI-API]
- Endpoint: GET /models
- Base URL: https://api.openai.com/v1
- Auth: Bearer token from OPENAI_API_KEY environment variable

### File Structure
[Source: architecture/source-tree.md]
- Handler location: `app/handlers/tags.py`
- Translation logic: `app/translators/response.py`
- Unit tests: `tests/unit/test_tags_handler.py`
- Integration tests: `tests/integration/test_tags_integration.py`

### Previous Story Context
All infrastructure from Epic 2 is complete:
- FastAPI app structure is set up
- Logging utilities configured in `app/utils/logging.py`
- Error handling framework in `app/utils/errors.py`
- Testing infrastructure ready with pytest
- Code quality tools (black, flake8, mypy) configured

### Model Format Notes
[Source: From collected examples and SDK types]
- OpenAI returns models with fields: id, object, created, owned_by
- Ollama expects: name, modified_at, size, digest, details
- Will need to map/generate missing fields appropriately

### Testing Standards
[Source: architecture/test-strategy-and-standards.md]
- Test file location: `tests/unit/` and `tests/integration/`
- Use pytest 7.4.4 with pytest-asyncio 0.23.3
- Follow AAA pattern (Arrange, Act, Assert)
- Mock external dependencies in unit tests
- Use collected OpenAI responses from `references/openai-examples/models.json`
- Integration tests use real Ollama SDK against localhost:11434
- Minimum 80% code coverage required

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-07-31 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
<!-- To be filled by dev agent -->

### Debug Log References
<!-- To be filled by dev agent -->

### Completion Notes List
<!-- To be filled by dev agent -->

### File List
<!-- To be filled by dev agent -->

## QA Results
<!-- To be filled by QA agent -->